{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9251\n",
      "0.274636\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]), name=\"W\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"b\")\n",
    "\n",
    "y = tf.matmul(x,W) + b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "saver = tf.train.Saver({\"W\": W, \"b\": b})\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for _ in range(10000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "saver.save(sess, \"model.ckpt\")\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "print(cross_entropy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "0.9251\n",
      "0.274636\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y_\")\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]), name=\"W\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"b\")\n",
    "\n",
    "y = tf.matmul(x,W) + b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "saver = tf.train.Saver({\"W\": W, \"b\": b})\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, \"model.ckpt\")\n",
    "\n",
    "#for _ in range(10000):\n",
    "#    batch = mnist.train.next_batch(100)\n",
    "#    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "print(cross_entropy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files found.\n"
     ]
    }
   ],
   "source": [
    "#input_dir_path = r'C:\\Users\\ericd\\Desktop\\Projects\\playgrounds\\python-playground\\mnist_tfrecords'\n",
    "input_dir_path = r'C:\\Users\\ericd\\Desktop\\Projects\\playgrounds\\python-playground\\mnist'\n",
    "filenames = os.listdir(input_dir_path)\n",
    "filenames = list(filter(lambda f: f.lower().endswith('.tfrecord') or f.lower().endswith('.tfrecords'), filenames))\n",
    "filenames = list(map(lambda f: os.path.join(input_dir_path, f), filenames))\n",
    "print(\"{} files found.\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.30 (0.450 sec)\n",
      "Step 100: loss = 0.00 (0.039 sec)\n",
      "Step 200: loss = 0.14 (0.038 sec)\n",
      "Step 300: loss = 21.28 (0.042 sec)\n",
      "Step 400: loss = 0.00 (0.041 sec)\n",
      "Step 500: loss = 0.06 (0.038 sec)\n",
      "Step 600: loss = 0.34 (0.042 sec)\n",
      "Step 700: loss = 0.00 (0.042 sec)\n",
      "Step 800: loss = 0.14 (0.043 sec)\n",
      "Step 900: loss = 0.23 (0.039 sec)\n",
      "Step 1000: loss = 0.00 (0.039 sec)\n",
      "Step 1100: loss = 0.00 (0.039 sec)\n",
      "Step 1200: loss = 0.09 (0.038 sec)\n",
      "Step 1300: loss = 0.00 (0.039 sec)\n",
      "Step 1400: loss = 0.01 (0.039 sec)\n",
      "Step 1500: loss = 0.01 (0.043 sec)\n",
      "Step 1600: loss = 0.00 (0.040 sec)\n",
      "Step 1700: loss = 0.02 (0.039 sec)\n",
      "Step 1800: loss = 0.01 (0.038 sec)\n",
      "Step 1900: loss = 0.00 (0.038 sec)\n",
      "Step 2000: loss = 0.14 (0.039 sec)\n",
      "Step 2100: loss = 0.07 (0.038 sec)\n",
      "Step 2200: loss = 0.14 (0.042 sec)\n",
      "Step 2300: loss = 0.00 (0.039 sec)\n",
      "Step 2400: loss = 0.07 (0.040 sec)\n",
      "Step 2500: loss = 0.00 (0.039 sec)\n",
      "Step 2600: loss = 0.04 (0.039 sec)\n",
      "Step 2700: loss = 0.04 (0.041 sec)\n",
      "Step 2800: loss = 0.00 (0.039 sec)\n",
      "Step 2900: loss = 0.02 (0.040 sec)\n",
      "Step 3000: loss = 0.02 (0.039 sec)\n",
      "Step 3100: loss = 0.00 (0.045 sec)\n",
      "Step 3200: loss = 0.12 (0.040 sec)\n",
      "Step 3300: loss = 0.00 (0.043 sec)\n",
      "Step 3400: loss = 0.00 (0.045 sec)\n",
      "Step 3500: loss = 0.00 (0.046 sec)\n",
      "Step 3600: loss = 0.06 (0.039 sec)\n",
      "Step 3700: loss = 0.00 (0.038 sec)\n",
      "Step 3800: loss = 0.01 (0.040 sec)\n",
      "Step 3900: loss = 0.11 (0.041 sec)\n",
      "Step 4000: loss = 0.00 (0.038 sec)\n",
      "Step 4100: loss = 0.01 (0.038 sec)\n",
      "Step 4200: loss = 0.07 (0.039 sec)\n",
      "Step 4300: loss = 0.00 (0.039 sec)\n",
      "Step 4400: loss = 0.07 (0.041 sec)\n",
      "Step 4500: loss = 0.10 (0.039 sec)\n",
      "Step 4600: loss = 0.00 (0.038 sec)\n",
      "Step 4700: loss = 0.00 (0.039 sec)\n",
      "Step 4800: loss = 0.00 (0.041 sec)\n",
      "Step 4900: loss = 0.00 (0.041 sec)\n",
      "Step 5000: loss = 0.17 (0.039 sec)\n",
      "Step 5100: loss = 0.02 (0.039 sec)\n",
      "Step 5200: loss = 0.00 (0.046 sec)\n",
      "Step 5300: loss = 0.01 (0.039 sec)\n",
      "Step 5400: loss = 0.01 (0.039 sec)\n",
      "Step 5500: loss = 0.00 (0.042 sec)\n",
      "Step 5600: loss = 0.09 (0.039 sec)\n",
      "Step 5700: loss = 0.16 (0.039 sec)\n",
      "Step 5800: loss = 0.01 (0.039 sec)\n",
      "Step 5900: loss = 0.01 (0.039 sec)\n",
      "Done training for 10 epochs, 6000 steps.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=10)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image.set_shape([28*28])\n",
    "\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "    batch_size = 100\n",
    "    images, labels = tf.train.shuffle_batch(\n",
    "        [image, label], batch_size=batch_size, num_threads=2,\n",
    "        capacity=1000 + 3 * batch_size,\n",
    "        min_after_dequeue=1000)\n",
    "\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    y = tf.matmul(images, W) + b\n",
    "    \n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=onehot_labels, logits=y))\n",
    "\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init_op)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    try:\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "\n",
    "            _, labels_values, loss_value = sess.run([train_op, labels, loss])\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            step += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (10, step))\n",
    "    finally:\n",
    "        print('Finished.')\n",
    "        coord.request_stop()\n",
    "\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
